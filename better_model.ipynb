{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c80586bb-5545-4eb6-b23c-d04d60c26b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated and will be removed in a future release\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.downloader as api\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d5a4e4-bb56-4147-8b6d-389d554720cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath, sep_type):\n",
    "    \"\"\"Load dataset from file and return dataframe.\"\"\"\n",
    "    return pd.read_csv(filepath, sep=f'{sep_type}', names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce3f563-a317-42fa-b1ef-99c6ad6da54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_email(email):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Loại bỏ ký tự không mong muốn\n",
    "    email_cleaned = re.sub('[^a-zA-Z]', ' ', email)\n",
    "    # Chuyển thành chữ thường và tách từ\n",
    "    email_cleaned = email_cleaned.lower().split()\n",
    "    # Lemmatization\n",
    "    email_cleaned = [lemmatizer.lemmatize(word) for word in email_cleaned]\n",
    "    email_cleaned = ' '.join(email_cleaned)\n",
    "    \n",
    "    return email_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97744b7-8776-476f-8812-4614b108ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word2vec(words):\n",
    "    \"\"\"Chuyển văn bản thành vector trung bình của Word2Vec (áp dụng cho cả huấn luyện và dự đoán).\"\"\"\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv.index_to_key]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7332a19f-a77f-4c34-9265-74d14959f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(corpus):\n",
    "    words = []\n",
    "    for sent in corpus:\n",
    "        sent_token = sent_tokenize(sent)\n",
    "        for sent in sent_token:\n",
    "            words.append(simple_preprocess(sent))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7978c3f-a060-4121-a0f2-cad09d0c105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "messages = load_data('SMSSpamCollection.txt', '\\t')\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "corpus = [preprocess_email(text) for text in messages['message']]\n",
    "\n",
    "words = tokenizer(corpus)\n",
    "\n",
    "model = gensim.models.Word2Vec(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "657fa21c-f377-48b7-9c6d-3f5f507f54da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16094023  0.24429367  0.12193104  0.11323108  0.08375572 -0.4852251\n",
      "  0.18646455  0.46235788 -0.26611784 -0.16720085 -0.11719898 -0.3821741\n",
      " -0.0273889   0.09587178  0.19931193 -0.15951428  0.08937047 -0.30973163\n",
      " -0.04517631 -0.5176502   0.20041043  0.10936268  0.10300862 -0.22962676\n",
      " -0.04926636 -0.0127106  -0.20762627 -0.1821898  -0.25954726  0.03918045\n",
      "  0.2960206   0.02348683  0.10209922 -0.1774608  -0.10078663  0.4112987\n",
      "  0.10113541 -0.11921144 -0.15400642 -0.44177574  0.10847908 -0.23453476\n",
      " -0.17050324 -0.00461487  0.15402652 -0.00668712 -0.14629723 -0.03287847\n",
      "  0.21666364  0.1222676   0.1606414  -0.21153969 -0.04806506  0.06725422\n",
      " -0.10126086  0.09384183  0.15467389 -0.02278042 -0.40434954  0.14738019\n",
      "  0.01313257  0.15763931 -0.00119324 -0.07919238 -0.30182502  0.2780778\n",
      "  0.07563531  0.23247293 -0.32184085  0.39147112 -0.26944056  0.1782777\n",
      "  0.40150598 -0.10626078  0.35069466  0.08477375  0.1115392  -0.09624092\n",
      " -0.18632297  0.07318625 -0.22273752 -0.10988794 -0.25339857  0.46456683\n",
      " -0.14549637 -0.00436382  0.02601036  0.23582245  0.34511286  0.0643945\n",
      "  0.34568313  0.17495446  0.01220995  0.06161007  0.42380372  0.19540498\n",
      "  0.13094227 -0.20651445  0.16686979  0.02263221]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for i in range(len(words)):\n",
    "    X.append(avg_word2vec(words[i]))\n",
    "    \n",
    "print(X[0])\n",
    "\n",
    "# Dependent feature\n",
    "y = messages[list(map(lambda x: len(x) > 0, corpus))]\n",
    "y = pd.get_dummies(y['label']).astype(int)\n",
    "y = y.iloc[:, 0].values\n",
    "\n",
    "\n",
    "df_list = [pd.DataFrame(X[i].reshape(1, -1)) for i in range(len(X))]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df['Output'] = y\n",
    "df.dropna(inplace = True)\n",
    "\n",
    "X = df.drop('Output', axis=1)\n",
    "\n",
    "y = df['Output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f043b57a-9855-4fcf-b3fb-ac944cdc07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8151bb9-faa3-48d1-8a0c-3b23081670c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bed7d48-edd5-4881-8ca6-6c45222b8b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9658886894075404"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95ea615a-82e0-4789-b0d8-f28ed93eebc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.85      0.88       171\n",
      "           1       0.97      0.99      0.98       943\n",
      "\n",
      "    accuracy                           0.97      1114\n",
      "   macro avg       0.95      0.92      0.93      1114\n",
      "weighted avg       0.97      0.97      0.97      1114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0067685f-df2e-479e-98cd-1fe422f246c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đây là email spam.\n",
      "Đây là email spam.\n"
     ]
    }
   ],
   "source": [
    "# Ví dụ email mới\n",
    "new_email = \"Hey mohan, can we get together to watch footbal game tomorrow?\"\n",
    "\n",
    "# Bước 1: Xử lý dữ liệu email mới\n",
    "def predict_email(email, classifier):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Loại bỏ ký tự không mong muốn\n",
    "    email_cleaned = re.sub('[^a-zA-Z]', ' ', email)\n",
    "    # Chuyển thành chữ thường và tách từ\n",
    "    email_cleaned = email_cleaned.lower().split()\n",
    "    # Lemmatization\n",
    "    email_cleaned = [lemmatizer.lemmatize(word) for word in email_cleaned]\n",
    "    \n",
    "    print(email_cleaned)\n",
    "\n",
    "    email_vector = avg_word2vec(email_cleaned)\n",
    "    email_vector = email_vector.reshape(1, -1)\n",
    "    predicted_label = classifier.predict(email_vector)\n",
    "\n",
    "    if predicted_label[0] == 0:\n",
    "        print(\"Đây là email spam.\")\n",
    "    else:\n",
    "        print(\"Đây là email bình thường (ham).\")\n",
    "\n",
    "predict_email(new_email, classifier)\n",
    "predict_email('Get rich quick with our investment opportunity! Join now and start earning big profits!', classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
